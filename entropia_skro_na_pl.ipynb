{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNQzUqNOTOxu"
      },
      "source": [
        "copyright 2020: *Władysław Skarbek, Warsaw University of Technology*<br>\n",
        "(Jupyter notebook addressed to students of: EADIR, GSN, MATMU)\n",
        "$\\newcommand{\\xx}[2]{(#1)(#2)}$\n",
        "$\\newcommand{\\bb}[1]{\\mathbb{#1}}$\n",
        "$\\newcommand{\\cl}[1]{\\mathcal{#1}}$\n",
        "$\\newcommand{\\tp}[1]{{#1}^{\\intercal}}$\n",
        "$\\newcommand{\\tr}[1]{\\text{trace}\\left[#1\\right]}$\n",
        "$\\newcommand{\\inv}[1]{\\in\\bb{R}^{#1}}$\n",
        "$\\newcommand{\\inm}[2]{\\in\\bb{R}^{#1\\times#2}}$\n",
        "$\\newcommand{\\invc}[1]{\\in\\bb{C}^{#1}}$\n",
        "$\\newcommand{\\inmc}[2]{\\in\\bb{C}^{#1\\times#2}}$\n",
        "$\\newcommand{\\rbox}[2]{\\mathrel{\\raise{#1}{#2}}}$\n",
        "$\\newcommand{\\xconv}[5]{\n",
        "\\begin{array}[]{r}{\\tiny{#4\\ }}\\\\\\raise{2.5pt}{{\\tiny{#3}}}\\end{array}\n",
        "\\kern{-4pt}\n",
        "\\begin{array}[]{l}{\\LARGE\\bb{C}}\\kern{-6.5pt}\\raise{4.5pt}{{\\tiny{#5}}}\\end{array}\n",
        "\\kern{0pt}\n",
        "\\begin{array}[]{l}{\\tiny {#2}}\\\\\\raise{2.5pt}{{\\tiny{#1}}}\\end{array}}$\n",
        "$\\newcommand{\\xdense}[5]{\n",
        "\\begin{array}[]{r}{\\tiny{#4\\ }}\\\\\\raise{1.5pt}{{\\tiny{#3}}}\\end{array}\n",
        "\\kern{-4pt}\n",
        "\\begin{array}[]{l}{\\LARGE\\bb{F}}\\kern{-4.5pt}\\raise{1.5pt}{{\\tiny{#5}}}\\end{array}\n",
        "\\kern{2pt}\n",
        "\\begin{array}[]{l}{\\tiny {#2}}\\\\\\raise{1.5pt}{{\\tiny{#1}}}\\end{array}}$\n",
        "$\\newcommand{\\xpool}[5]{\n",
        "\\begin{array}[]{r}{\\tiny{#4\\ }}\\\\\\raise{1.5pt}{{\\tiny{#3}}}\\end{array}\n",
        "\\kern{-3pt}\n",
        "\\begin{array}[]{l}{\\LARGE\\bb{P}}\\kern{-6.5pt}\\raise{0.8pt}{{\\tiny{#5}}}\\end{array}\n",
        "\\kern{0pt}\n",
        "\\begin{array}[]{l}{\\tiny {#2}}\\\\\\raise{1.5pt}{{\\tiny{#1}}}\\end{array}}$\n",
        "$\\newcommand{\\xinp}[5]{\n",
        "\\begin{array}[]{r}{\\tiny{#4\\ }}\\\\\\raise{1.5pt}{{\\tiny{#3}}}\\end{array}\n",
        "\\kern{-4pt}\n",
        "\\begin{array}[]{l}{\\LARGE\\cl{I}}\\kern{-4.5pt}\\raise{1.5pt}{{\\tiny{#5}}}\\end{array}\n",
        "\\kern{2pt}\n",
        "\\begin{array}[]{l}{\\tiny {#2}}\\\\\\raise{1.5pt}{{\\tiny{#1}}}\\end{array}}$\n",
        "$\\newcommand{\\xdrop}[5]{\n",
        "\\begin{array}[]{r}{\\tiny{#4\\ }}\\\\\\raise{1.5pt}{{\\tiny{#3}}}\\end{array}\n",
        "\\kern{-4pt}\n",
        "\\begin{array}[]{l}{\\LARGE\\bb{D}}\\kern{-4.5pt}\\raise{1.5pt}{{\\tiny{#5}}}\\end{array}\n",
        "\\kern{0.5pt}\n",
        "\\begin{array}[]{l}{\\tiny {#2}}\\\\\\raise{1.5pt}{{\\tiny{#1}}}\\end{array}}$\n",
        "$\\newcommand{\\xmerge}[5]{\n",
        "\\begin{array}[]{r}{\\tiny{#4\\ }}\\\\\\raise{1.5pt}{{\\tiny{#3}}}\\end{array}\n",
        "\\kern{-4pt}\n",
        "\\begin{array}[]{l}{\\LARGE\\bb{M}}\\kern{-4.5pt}\\raise{1.5pt}{{\\tiny{#5}}}\\end{array}\n",
        "\\kern{2pt}\n",
        "\\begin{array}[]{l}{\\tiny {#2}}\\\\\\raise{1.5pt}{{\\tiny{#1}}}\\end{array}}$\n",
        "$\\newcommand{\\xgeneral}[5]{\n",
        "\\begin{array}[]{r}{\\tiny{#4\\ }}\\\\\\raise{1.5pt}{{\\tiny{#3}}}\\end{array}\n",
        "\\kern{-4pt}\n",
        "\\begin{array}[]{l}{\\LARGE\\bb{Q}}\\kern{-9.0pt}\\raise{4.5pt}{{\\tiny{#5}}}\\end{array}\n",
        "\\kern{2pt}\n",
        "\\begin{array}[]{l}{\\tiny {#2}}\\\\\\raise{1.5pt}{{\\tiny{#1}}}\\end{array}}$\n",
        "$\\def\\ds{\\displaystyle}$\n",
        "$\\def\\ass{\\leftarrow}$\n",
        "$\\def\\od#1#2{\\nabla_{#2}#1}$\n",
        "$\\def\\tod#1#2{\\tp{\\nabla}_{#2}{#1}}$\n",
        "$\\def\\cl#1{{\\cal#1}}$\n",
        "$\\def\\sp#1#2{\\frac{\\partial#1}{\\partial#2}}$\n",
        "$\\def\\eqd{\\doteq}$\n",
        "$\\def\\ra{\\rightarrow}$\n",
        "$\\def\\lra{\\longrightarrow}$\n",
        "$\\def\\ovra#1{\\overset{#1}{\\lra}}$\n",
        "$\\def\\dra{\\overset{\\circ}{\\lra}}$\n",
        "$\\def\\xeq#1{\\overset{#1}{=}}$\n",
        "$\\def\\ov#1{\\overline{#1}}$\n",
        "$\\def\\dotp#1#2{\\left\\langle#1,#2\\right\\rangle}$\n",
        "$\\def\\th#1{\\ov{#1}^{\\intercal}}$\n",
        "$\\def\\rv#1{\\widetilde{#1}}$\n",
        "$\\def\\vars#1#2{\\mathtt{var}_{#1}{\\left[#2\\right]}}$\n",
        "$\\def\\pmodd#1{\\kern{-1mm}\\pmod{#1}}$\n",
        "$\\def\\pdt#1#2#3{\\frac{\\partial^2 #1}{\\partial #2\\partial #3}}$\n",
        "$\\def\\diag#1{\\mathtt{diag}\\left[#1\\right]}$\n",
        "$\\def\\rank#1{\\mathtt{rank}\\left[#1\\right]}$\n",
        "$\\def\\tr#1{\\mathtt{tr}\\left[#1\\right]}$\n",
        "$\\def\\colorr#1#2{\\color{#1}{#2}}$\n",
        "$\\def\\kernn#1{\\kern{#1pt}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCbzt_4Ego1A"
      },
      "source": [
        "#Entropia krzyżowa (skrośna)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1GkrvLBdMSM"
      },
      "source": [
        "##Dywergencja Kullbacka-Leiblera (KL)\n",
        "\n",
        "Dywergencja KL $D_{KL}(p||q)$ to asymetryczna miara dyskryminacyjna dla rozkładów prawdopodobieństwa $p,q:\\cl{A}\\ra\\big[0,1\\big]$:\n",
        "- *przypadek dyskretny*:\n",
        "$D_{KL}(p||q) \\eqd \\ds\\sum_{x\\in\\cl{A}} p(x)\\log\\frac{p(x)}{q(x)}$,\n",
        "- *przypadek ciągły*:\n",
        "$D_{KL}(p||q) \\eqd \\ds\\int_{\\cl{A}} p(x)\\log\\frac{p(x)}{q(x)}dx$.\n",
        "\n",
        "*Uwaga:*<br>Stosujemy następujący zapis dla logarytmów: $\\ln t\\eqd\\log_e t,\\ \\log t\\eqd \\log_2 t\\ .$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvfurmKNljDx"
      },
      "source": [
        "###*Kod KL: przypadek dyskretny*\n",
        "Przjmujemy domyślny zakres dla losowych indeksów dal $K$ klas: $\\cl{A} \\eqd \\{0,\\dots,K-1\\}$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLm1RmAOn3iz"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcMXK9qtpFtf"
      },
      "source": [
        "eps = 1e-30\n",
        "def KL(p,q,base=2,eps=eps):\n",
        "  if base==np.e: return np.sum(p*(np.log(eps+p/(q+eps))))\n",
        "  else: return np.sum(p*(np.log2(eps+p/(q+eps))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92hU9nw25U94"
      },
      "source": [
        "Testowanie"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnzRLRdJnnEo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e77dc86-79ff-411d-d651-f7ed4b41cd5d"
      },
      "source": [
        "K = 10; print('number of classes:',K)\n",
        "print('epsilon regularizer:', eps)\n",
        "lbase = np.e # 2\n",
        "print('logarithm base:',lbase)\n",
        "p = np.random.rand(K); p /= np.sum(p)+eps\n",
        "q = np.random.rand(K); q /= np.sum(q)+eps\n",
        "print('p:',p); print('q:',q)\n",
        "print('Kullback-Leibler divergence KL(p,q) for probability distributions p,q:',KL(p,q,base=lbase))\n",
        "print('Kullback-Leibler divergence KL(q,p) for probability distributions q,p:',KL(q,p,base=lbase))\n",
        "print('KL(p,p):',KL(p,p),'KL(q,q):',KL(q,q))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of classes: 10\n",
            "epsilon regularizer: 1e-30\n",
            "logarithm base: 2.718281828459045\n",
            "p: [0.00794213 0.12628191 0.00923124 0.06326613 0.1601021  0.16799011\n",
            " 0.1105824  0.09152709 0.13877271 0.12430418]\n",
            "q: [0.09525451 0.00551859 0.11034399 0.12148165 0.13248914 0.10418057\n",
            " 0.18861651 0.11264091 0.03207674 0.09739739]\n",
            "Kullback-Leibler divergence KL(p,q) for probability distributions p,q: 0.5775126255333463\n",
            "Kullback-Leibler divergence KL(q,p) for probability distributions q,p: 0.55088808969596\n",
            "KL(p,p): 0.0 KL(q,q): 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grgBCfz7IzQY"
      },
      "source": [
        "Eksperymenty wskazują na następujące własności: dywergencja KL jest nieujemna, niesymetryczna i równa się zero gdy rozkłady są identyczne.\n",
        "Czy to jest prawdą w ogólności?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfzpByVAddGl"
      },
      "source": [
        "###Kluczowe nierówności w teorii informacji\n",
        "\n",
        "Logarytm, funkcja taka jak $\\ln t$, jest różniczkowalny, ściśle wklęsłą funkcją w dziedzinie $t>0$. A więc logarytm ma wykres, za wyjątkiem punktu styczności,  poniżej każdej stycznej. Ponieważ logarytm znika w punkcie $t=1$ z nachyleniem jednostkowym (pochodna $\\frac{d\\ ln(t)}{d\\ t}=\\left.\\frac{1}{t}\\right|_{t=1}=1$) to styczna ma równanie $u=t-1.$  Dlatego dla $t>0:$\n",
        "$$\n",
        "\\ln t \\leq t-1\\quad\\lra\\quad\n",
        "\\log t \\leq (t-1)\\log e\\quad \\lra\\quad\n",
        "-\\log t \\geq (1-t)\\log e\n",
        "$$\n",
        "Równość zachodzi wtedy i tylko wtedy $t=1$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wabI5MAAeVXZ"
      },
      "source": [
        "####Dolne ograniczenie dywergencji KL\n",
        "Podstawiając do nierówności $-\\log t\\geq(1-t)\\log e$  jako $t\\eqd\\frac{q(x)}{p(x)}$ otrzymujemy dolne ograniczenie $p(x)\\log\\frac{p(x)}{q(x)}:$\n",
        "$$\\log\\frac{p(x)}{q(x)} = -\\log\\frac{q(x)}{p(x)} \\geq \\frac{p(x)-q(x)}{p(x)}\\cdot\\log e \\quad\\lra\\quad p(x)\\log\\frac{p(x)}{q(x)} \\geq (p(x)-q(x))\\log e\n",
        "$$\n",
        "\n",
        "Równość $p(x)\\log\\frac{p(x)}{q(x)} = (p(x)-q(x))\\log e$ zachodzi tylko gdy $t=1$, tj. wtedy i tylko wtedy gdy $p(x)=q(x)$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DjwYHTYe6VQ"
      },
      "source": [
        "####Dywergencja KL jest zawsze nieujemna\n",
        "Dla dowolnych rozkładów prawdopodobieństwa $p,q$ określonych w tej samej dziedzinie:\n",
        "$$\n",
        "\\boxed{\n",
        "\\begin{array}{l}\n",
        "D_{KL}(p||q)\\geq 0\\\\[3pt]\n",
        "D_{KL}(p||q)=0 \\iff p=q\n",
        "\\end{array}\n",
        "}\n",
        "$$\n",
        "\n",
        "**Ćwiczenie**  Uzasadnij powyższe własności dywergencji KL: nieujemność i identyczność nierozróżnialnych.<br>\n",
        "*Wskazówka:* Skorzystaj z powyżej wyprowadzonej nierówności<br>\n",
        " $p(x)\\log\\frac{p(x)}{q(x)} \\geq (p(x)-q(x))\\log e$<br>\n",
        " oraz z własności jedynki<br>\n",
        " $\\ds\\sum_{x\\in\\cl{A}}p(x)=1$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8sxDNhIgNAv"
      },
      "source": [
        "###Entropia a dywergencja KL\n",
        "\n",
        "Entropię $H[p]$ rozkładu $p:\\cl{A}\\ra[0,1]$ jest wartością oczekiwaną miary informacji $\\log\\frac{1}{p(x)},$ $x\\in\\cl{A}$:\n",
        "$$\n",
        "H(p)\\eqd\\ds\\sum_{x\\in\\cl{A}}p(x)\\log\\frac{1}{p(x)}\n",
        "$$\n",
        "Niech $r^{\\ast}$ będzie rozkładem równomiernym: $r^{\\ast}(x)\\eqd\\frac{1}{|\\cl{A}|},$ dla dowolnego  $x\\in\\cl{A}$.\n",
        "\n",
        "**Ćwiczenie**\n",
        "\n",
        "Uzasadnij dla dowolnego rozkładu $p$ na dziedzinie $\\cl{A}:$\n",
        "$$\n",
        "\\begin{array}{rcl}\n",
        "H(p) & = & \\log|\\cl{A}|-D_{KL}(p||r^{\\ast})\\\\[5pt]\n",
        "H(r^{\\ast}) & = & \\log|\\cl{A}|\\\\[5pt]\n",
        "D_{KL}(p||r^{\\ast}) & = &  H(r^{\\ast})-H(p)\n",
        "\\end{array}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_8-W1AhHefZ"
      },
      "source": [
        "###Entropia skrośna a dywergencja KL\n",
        "\n",
        "Entropię skrośną $H(p,q)$  rozkładów $p,q:\\cl{A}\\ra[0,1]$ określamy jako wartość oczekiwaną informacji $\\log\\frac{1}{q(x)}$ w rozkładzie $q$  uśrednianą zgodnie z rozkładem $p$:\n",
        "$$\n",
        "H(p,q)\\eqd\\ds\\sum_{x\\in\\cl{A}}p(x)\\log\\frac{1}{q(x)}\n",
        "$$\n",
        "\n",
        "**Ćwiczenie**\n",
        "\n",
        "Uzasadnij, że dla dowolnych rozkładów $p,q$ on $\\cl{A}:$\n",
        "$$D_{KL}(p||q) = H(p,q) - H(p)\\ \\text{ oraz }\\ H(p,q) = D_{KL}(p||q) + H(p)$$\n",
        "\n",
        "*Wnioski:*\n",
        "- Jesli $p$ jest docelowym rozkładem prawdopopdobieństwa a rozkład $q$ jest predykcją is tego rozkładu wtedy  entropia skrośna $H(p,q)$ może być stosowana na równi z optymalizacją dywergencji KL $D_{KL}(p||q)$.\n",
        "- Jeśli docelowy rozkład jest skupiony na indeksie $k$, tj. gdy $p(x)\\eqd\\delta(x-k)$, to $H(p)=0$ i wtedy błąd predykcji równa się entropii skrośnej, która z kolei równa się informacji indeksu $k$-tej klasy:\n",
        "$$p(x)\\eqd\\delta(x-k)\\quad \\lra\\quad D_{KL}(p||q)=H(p,q)=\\log\\frac{1}{q(k)}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNwNYBkQPQzm"
      },
      "source": [
        "Kod entropii i entropii skrośnej"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1CjWlBBPPte"
      },
      "source": [
        "def entropy(p): return np.sum(p*np.log2(eps+1./(eps+p)))\n",
        "def cross_entropy(p,q): return np.sum(p*np.log2(eps+1./(eps+q)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfaDqGs3P5RY"
      },
      "source": [
        "Testowanie"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpgtJWliP7XF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "167a9c2e-59c7-4b6b-80b8-327ddbbff0bd"
      },
      "source": [
        "K = 10; print('number of classes:',K)\n",
        "print('epsilon regularizer:', eps)\n",
        "p = np.random.rand(K); p /= np.sum(p)+eps\n",
        "q = np.random.rand(K); q /= np.sum(q)+eps\n",
        "print('p:',p); print('q:',q)\n",
        "print('Entropy H(p) for probability distribution p:',entropy(p))\n",
        "print('Cross entropy H(p,q) for probability distributions p,q:',cross_entropy(p,q))\n",
        "print('KL(p,q):',KL(p,q))\n",
        "print('check KL(p,q) - (H(p,q) - H(p)) = 0?:',KL(p,q)-cross_entropy(p,q)+entropy(p))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of classes: 10\n",
            "epsilon regularizer: 1e-30\n",
            "p: [0.07185327 0.04543817 0.15124233 0.12249065 0.05451429 0.07568778\n",
            " 0.09867479 0.13907186 0.12432914 0.11669771]\n",
            "q: [0.0019638  0.15676857 0.04356393 0.21716675 0.0761604  0.19451716\n",
            " 0.05514493 0.16371119 0.07358279 0.01742046]\n",
            "Entropy H(p) for probability distribution p: 3.23058213591542\n",
            "Cross entropy H(p,q) for probability distributions p,q: 4.0279745628793595\n",
            "KL(p,q): 0.7973924269639393\n",
            "check KL(p,q) - (H(p,q) - H(p)) = 0?: -4.440892098500626e-16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3KZcqgVzyH6"
      },
      "source": [
        "###Normalizacja *SoftMax* i entropia skrośna"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFPGSh-Puld0"
      },
      "source": [
        "Niech $y\\eqd F(x)\\inv{K}$ będzie wektorem wyników dla wejścia $x$, które reprezentuje klasę o indeksie $k^{\\ast}\\eqd t(x)\\in[K].$<br>\n",
        "Wtedy funkcja strat w kombinacji *SoftMax+CrossEntropy* jest definiowana następująco:\n",
        "- *Wynik wejściowy* $y$: $y[k]$ - artość wynikowa dla klasy $k$, $k\\in[K]$.\n",
        "- *Docelowy rozkład prawdopodobieństwa* $q^{\\ast}$: $q^{\\ast}[k]\\eqd \\delta[k-k^{\\ast}],\\ k\\in[K]$.\n",
        "\n",
        "*Normalizacja SoftMax*\n",
        "\n",
        "Dla $k\\in[K]$:\n",
        "1. $\\ds z[k]\\eqd e^{y[k]}$ $-$ przechodzimy do skali, w której wynik $y[k]$ ma charakter logarytmiczny,\n",
        "1. $\\ds p_y[k]\\eqd \\frac{z[k]}{\\ds\\sum_{i\\in[K]}z[i]}$ $-$ normalizacja wyniku do rozkładu prawdopodobieństwa\n",
        "\n",
        "*Entropia skrośna*\n",
        "\n",
        "- *Funkcja strat* = $\\frac{1}{\\ln 2}\\cdot$ entropia skrośna rozkładów $q^{\\ast}$ oraz $p_y$:\n",
        "$$\\cl{E}(y) \\eqd -\\ln p_y[k^{\\ast}]$$\n",
        "- Jawna formuła funkcji strat względem wektora wyników $y\\inv{K}:$\n",
        "$$\n",
        "\\cl{E}(y) = -\\ln\\frac{e^{y[k^{\\ast}]}}{\\ds\\sum_{k\\in[K]}e^{y[k]}}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hY84H6IRv0TI"
      },
      "source": [
        "*Uwagi*:\n",
        "- Faktycznie powyższa funkcja strat nie jest stricte entropią skrośną, ale jej skalowalną przez $\\frac{1}{\\log e}$ wersją.\n",
        "- Ponieważ wyrażenie $-\\ln p_y[k^{\\ast}]=\\left(-\\log p_y[k^{\\ast}]\\right)\n",
        "\\cdot\\frac{1}{\\log e}$\n",
        "jest proporcjonalne do miary informacji w sensie Shannona, można interpretować tę funkcję strat jako informację Shannona *Shannon information* indeksu $k^{\\ast}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_NXtPNKsVGz"
      },
      "source": [
        "Kod dla *SoftMax*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5_lrKOnsep7"
      },
      "source": [
        "def softmax(y): p = np.exp(y); return p/np.sum(p)\n",
        "def bayes_principle(y): p = softmax(y); k = np.argmax(p); return k, p[k]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6oq7qMQvvrr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7624f285-7e57-47ca-f981-32fe3660e0d8"
      },
      "source": [
        "K = 10; y = np.random.randn(K)\n",
        "print('number of classes:',K); print('random scores y:',y)\n",
        "p = softmax(y); print('their SoftMax normalization probabilities p:',p)\n",
        "print('check property of one:',np.sum(p))\n",
        "k, p_k = bayes_principle(y)\n",
        "print('class by Bayesian principle:',k,'\\nwith probability:',p_k)\n",
        "print('class from scores:',np.argmax(y))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of classes: 10\n",
            "random scores y: [ 0.48157395 -1.21517382 -1.20813598 -0.73438881  0.60160088  0.34793682\n",
            "  0.99714601  0.39194759 -0.05644228 -1.08403581]\n",
            "their SoftMax normalization probabilities p: [0.14187525 0.0260027  0.02618635 0.0420553  0.1599682  0.12412772\n",
            " 0.23758375 0.12971267 0.08284173 0.02964633]\n",
            "check property of one: 1.0\n",
            "class by Bayesian principle: 6 \n",
            "with probability: 0.23758375260124703\n",
            "class from scores: 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN73yKO_v5nc"
      },
      "source": [
        "###Gradient funkcji *SoftMax+KL*\n",
        "**Twierdzenie** (o gradiencie Softamx+KL)<br>\n",
        "Zakładając powyższą definicję funkcji strat $\\cl{E}(y), y\\inv{K},$ jej gradient względem wyniku $y$ może byc wyrażony przez różnicę między rozkładem *SoftMax*  $p_y$ a rozkładem docelowym $q^{\\ast}$:\n",
        "$\n",
        "\\boxed{\\od{\\cl{E}}{y} = p_y-q^{\\ast}}\n",
        "$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thy5u-5DFHRr"
      },
      "source": [
        "*Dowód:*<br>\n",
        "Zaczynamy od policzenia pochodnych $\\sp{\\cl{E}(y)}{y[i]}$ for any $i\\in[K]$:\n",
        "$$\n",
        "\\begin{array}{l}\n",
        "\\ds\\sp{\\cl{E}}{y[i]} =\n",
        "\\sp{}{y[i]}\\left[-\\ln\\frac{e^{y[k^{\\ast}]}}{\\ds\\sum_{k\\in[K]}e^{y[k]}}\\right]=\\\\[10pt]\n",
        "-\\left(\\frac{\\ds\\sum_{k\\in[K]}e^{y[k]}}{e^{y[k^{\\ast}]}}\\right)\\cdot\n",
        "\\frac{\\ds\\sp{e^{y[k^{\\ast}]}}{y[i]}\\cdot\n",
        "\\left(\\ds\\sum_{k\\in[K]}e^{y[k]}\\right)-e^{y[k^{\\ast}]}\\cdot\\sp{}{y[i]}\\left(\\ds\\sum_{k\\in[K]}e^{y[k]}\\right)}\n",
        "{\\left(\\ds\\sum_{k\\in[K]}e^{y[k]}\\right)^2}\n",
        "\\end{array}\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVS8UIB_MQpH"
      },
      "source": [
        "Teraz, rozważmy przypadek $i\\neq k^{\\ast}$:\n",
        "$$\n",
        "\\begin{array}{l}\n",
        "\\ds\\sp{\\cl{E}}{y[i]} =\n",
        "-\\left(\\frac{\\ds\\sum_{k\\in[K]}e^{y[k]}}{e^{y[k^{\\ast}]}}\\right)\\cdot\n",
        "\\frac{\\overbrace{\\ds\\sp{e^{y[k^{\\ast}]}}{y[i]}}^{\\ds 0}\\cdot\n",
        "\\left(\\ds\\sum_{k\\in[K]}e^{y[k]}\\right)-e^{y[k^{\\ast}]}\\cdot\n",
        "\\overbrace{\\ds\\sp{}{y[i]}\\left(\\ds\\sum_{k\\in[K]}e^{y[k]}\\right)}^{\\ds e^{y[i]}}\n",
        "}\n",
        "{\\left(\\ds\\sum_{k\\in[K]}e^{y[k]}\\right)^2}\\\\[5pt]\n",
        "\\lra\\quad \\ds\\sp{\\cl{E}}{y[i]} = \\frac{e^{y[i]}}{\\ds\\sum_{k\\in[K]}e^{y[k]}}\n",
        "\\quad\\lra\\quad \\ds\\sp{\\cl{E}}{y[i]} =  p_y[i]\\ \\text{ if } i\\neq k^{\\ast}\n",
        "\\end{array}\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmBpc1XAP-le"
      },
      "source": [
        "Końcowy przypadek $i=k^{\\ast}$:\n",
        "$$\n",
        "\\begin{array}{l}\n",
        "\\ds\\sp{\\cl{E}}{y[k^{\\ast}]} =\n",
        "-\\left(\\frac{\\ds\\sum_{k\\in[K]}e^{y[k]}}{e^{y[k^{\\ast}]}}\\right)\\cdot\n",
        "\\frac{\\overbrace{\\ds\\sp{e^{y[k^{\\ast}]}}{y[k^{\\ast}]}}^{\\ds e^{y[k^{\\ast}]}}\\cdot\n",
        "\\left(\\ds\\sum_{k\\in[K]}e^{y[k]}\\right)-e^{y[k^{\\ast}]}\\cdot\n",
        "\\overbrace{\\ds\\sp{}{y[k^{\\ast}]}\\left(\\ds\\sum_{k\\in[K]}e^{y[k]}\\right)}^{\\ds e^{y[k^{\\ast}]}}\n",
        "}\n",
        "{\\left(\\ds\\sum_{k\\in[K]}e^{y[k]}\\right)^2}\\\\[5pt]\n",
        "\\lra\\quad \\ds\\sp{\\cl{E}}{y[k^{\\ast}]} = \\frac{e^{y[k^{\\ast}]}}{\\ds\\sum_{k\\in[K]}e^{y[k]}}-1\n",
        "\\quad\\lra\\quad \\ds\\sp{\\cl{E}}{y[k^{\\ast}]} =  p_y[k^{\\ast}]-1\\ \\text{ if } i\\neq k^{\\ast}\n",
        "\\end{array}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aV4EeyedTd18"
      },
      "source": [
        "Podsumowując:\n",
        "$$\n",
        "\\begin{array}{l}\n",
        "\\ds\\sp{\\cl{E}}{y[i]}\n",
        "=\n",
        "\\left\\{\n",
        "\\begin{array}{ll}\n",
        "   p_y[i] = p_y[i]-\\delta(i-k^{\\ast}) = p_y[i] - q^{\\ast}[i]  & \\text{ if } i\\neq k^{\\ast}\\\\\n",
        "   p_y[i]-1 = p_y[i] - \\delta(i-k^{\\ast}) = p_y[i] - q^{\\ast}[i]& \\text{ if } i= k^{\\ast}\n",
        "\\end{array}\n",
        "\\right.\\\\[5pt]\n",
        "\\quad\\lra\\quad \\od{\\cl{E}}{y} = p_y-q^{\\ast}\\quad \\boxed{QED}\n",
        "\\end{array}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwfXEhS9xu1P"
      },
      "source": [
        "Kod gradientu dla *SoftMax+KL*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKi3Iyt2xtZd"
      },
      "source": [
        "def gradient_soft_KL(y,k): p = softmax(y); p[k] -= 1.; return p"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W70Zfea0xtZu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32ba18ae-13ab-4e32-cbac-ea335abf3856"
      },
      "source": [
        "K = 10; y = np.random.randn(K); k = np.random.randint(K)\n",
        "print('number of classes:',K); print('random class index:',k)\n",
        "print('random scores y:',y)\n",
        "g = gradient_soft_KL(y,k)\n",
        "print('gradient of SoftMax+KL loss:', g)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of classes: 10\n",
            "random class index: 0\n",
            "random scores y: [-1.24560355 -1.32991304  0.89132933  0.87014127  0.02608436 -0.93242469\n",
            "  2.65619314 -0.13985098 -0.98751607 -0.09632264]\n",
            "gradient of SoftMax+KL loss: [-0.98759091  0.01140577  0.10514726  0.10294283  0.04426157  0.01697273\n",
            "  0.61414166  0.03749402  0.01606297  0.03916212]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxKrARmh3wkM"
      },
      "source": [
        "###Gradient w TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aGgPZG_WBHg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "315075c1-5517-4aff-9327-8a986d8b1921"
      },
      "source": [
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "TAs5UD_6Fbng",
        "outputId": "23f6547a-2ffb-4497-d9b7-cf4f48181ce0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.14.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7XQzFIGIgNK"
      },
      "source": [
        "def KL_tf(p,q): return tf.math.reduce_sum(p*tf.math.log(eps+(p/(eps+q))))\n",
        "def softmax_tf(y): p = tf.math.exp(y); return p/tf.math.reduce_sum(p)\n",
        "def forward_tf(y,q): p = softmax_tf(y); return KL_tf(q,p)\n",
        "\n",
        "def CE_tf(y, q, eps=1e-15):\n",
        "    return -tf.reduce_sum(q * tf.math.log(eps + tf.nn.softmax(y)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJtfORQoI4b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "773b52f0-be8d-430d-edbf-7694b1c00dbd"
      },
      "source": [
        "q_star = np.zeros(len(y)); q_star[k] += 1.\n",
        "y_tf = tf.Variable(y); q_tf = tf.Variable(q_star)\n",
        "print('scores in numpy:',y)\n",
        "print('scores wrapped in tensorflow:',y_tf)\n",
        "print('class categoric vector:',q_tf)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "scores in numpy: [-1.24560355 -1.32991304  0.89132933  0.87014127  0.02608436 -0.93242469\n",
            "  2.65619314 -0.13985098 -0.98751607 -0.09632264]\n",
            "scores wrapped in tensorflow: <tf.Variable 'Variable:0' shape=(10,) dtype=float64, numpy=\n",
            "array([-1.24560355, -1.32991304,  0.89132933,  0.87014127,  0.02608436,\n",
            "       -0.93242469,  2.65619314, -0.13985098, -0.98751607, -0.09632264])>\n",
            "class categoric vector: <tf.Variable 'Variable:0' shape=(10,) dtype=float64, numpy=array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16uMHe7cKBnk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "762a27c3-5a04-4508-8397-0a0325ebb8ff"
      },
      "source": [
        "e = forward_tf(y_tf,q_tf)\n",
        "print('Softmax+KL error:',e)\n",
        "print('Note: in TensorFlow/Keras this loss function is implemented by:',\n",
        "      'tf.keras.losses.CategoricalCrossentropy')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Softmax+KL error: tf.Tensor(4.38932634632213, shape=(), dtype=float64)\n",
            "Note: in TensorFlow/Keras this loss function is implemented by: tf.keras.losses.CategoricalCrossentropy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCFWtWK9HI3S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c52b63e6-e242-4359-a5ee-fbedfff54da6"
      },
      "source": [
        "with tf.GradientTape() as gt: gt.watch(y_tf); e = forward_tf(y_tf,q_tf)\n",
        "de_dy = gt.gradient(e, y_tf)\n",
        "print('gradient of error by TensorFlow:',de_dy)\n",
        "print('difference of gradient from (p-q_star) formula with TensorFlow one:',\n",
        "      np.linalg.norm(g-de_dy)/K)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gradient of error by TensorFlow: tf.Tensor(\n",
            "[-0.98759091  0.01140577  0.10514726  0.10294283  0.04426157  0.01697273\n",
            "  0.61414166  0.03749402  0.01606297  0.03916212], shape=(10,), dtype=float64)\n",
            "difference of gradient from (p-q_star) formula with TensorFlow one: 1.1195352145514477e-17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ćwiczenie A\n",
        "\n",
        "Dostosuj powyższe funkcje do podstawy logarytmu naturalnego i podstawy 2.\n",
        "\n"
      ],
      "metadata": {
        "id": "PzfLAePjWCQM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xcZ67ZWleCQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ćwiczenie B\n",
        "\n",
        "Oblicz w TensorFlow dla danych z notatnika, gradient względem wejścia do jednostki CE (bez SoftMax), którą zdefiniuj dla TensorFlow jako funkcję `CE_tf`"
      ],
      "metadata": {
        "id": "6UBZ9zneeXdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "K = 10\n",
        "y = np.random.randn(K)\n",
        "k = np.random.randint(K)\n",
        "q_star = np.zeros(K)\n",
        "q_star[k] = 1.\n",
        "\n",
        "def CE_tf(y, q, eps=1e-15):\n",
        "    y_exp = tf.math.exp(y - tf.reduce_max(y))  # Stabilizacja numeryczna\n",
        "    return -tf.reduce_sum(q * tf.math.log(eps + y_exp / tf.reduce_sum(y_exp)))\n",
        "\n",
        "y_tf = tf.Variable(y, dtype=tf.float32)\n",
        "q_tf = tf.Variable(q_star, dtype=tf.float32)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "    tape.watch(y_tf)\n",
        "    loss = CE_tf(y_tf, q_tf)\n",
        "    gradient = tape.gradient(loss, y_tf)\n",
        "\n",
        "print('Gradient:', gradient.numpy())\n"
      ],
      "metadata": {
        "id": "-pHNoKfzeZ-5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08fa9a49-834a-474f-a855-43a639089ee2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient: [ 0.10807879  0.1993815   0.11376484  0.07071819  0.1681125  -0.95517194\n",
            "  0.08748703  0.14696656  0.0206524   0.04001016]\n"
          ]
        }
      ]
    }
  ]
}