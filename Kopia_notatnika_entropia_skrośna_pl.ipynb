{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zkradd/projekt-app/blob/main/Kopia_notatnika_entropia_skro%C5%9Bna_pl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNQzUqNOTOxu"
      },
      "source": [
        "copyright 2020: *Władysław Skarbek, Warsaw University of Technology*<br>\n",
        "(Jupyter notebook addressed to students of: EADIR, GSN, MATMU)\n",
        "$\\newcommand{\\xx}[2]{(#1)(#2)}$\n",
        "$\\newcommand{\\bb}[1]{\\mathbb{#1}}$\n",
        "$\\newcommand{\\cl}[1]{\\mathcal{#1}}$\n",
        "$\\newcommand{\\tp}[1]{{#1}^{\\intercal}}$\n",
        "$\\newcommand{\\tr}[1]{\\text{trace}\\left[#1\\right]}$\n",
        "$\\newcommand{\\inv}[1]{\\in\\bb{R}^{#1}}$\n",
        "$\\newcommand{\\inm}[2]{\\in\\bb{R}^{#1\\times#2}}$\n",
        "$\\newcommand{\\invc}[1]{\\in\\bb{C}^{#1}}$\n",
        "$\\newcommand{\\inmc}[2]{\\in\\bb{C}^{#1\\times#2}}$\n",
        "$\\newcommand{\\rbox}[2]{\\mathrel{\\raise{#1}{#2}}}$\n",
        "$\\newcommand{\\xconv}[5]{\n",
        "\\begin{array}[]{r}{\\tiny{#4\\ }}\\\\\\raise{2.5pt}{{\\tiny{#3}}}\\end{array}\n",
        "\\kern{-4pt}\n",
        "\\begin{array}[]{l}{\\LARGE\\bb{C}}\\kern{-6.5pt}\\raise{4.5pt}{{\\tiny{#5}}}\\end{array}\n",
        "\\kern{0pt}\n",
        "\\begin{array}[]{l}{\\tiny {#2}}\\\\\\raise{2.5pt}{{\\tiny{#1}}}\\end{array}}$\n",
        "$\\newcommand{\\xdense}[5]{\n",
        "\\begin{array}[]{r}{\\tiny{#4\\ }}\\\\\\raise{1.5pt}{{\\tiny{#3}}}\\end{array}\n",
        "\\kern{-4pt}\n",
        "\\begin{array}[]{l}{\\LARGE\\bb{F}}\\kern{-4.5pt}\\raise{1.5pt}{{\\tiny{#5}}}\\end{array}\n",
        "\\kern{2pt}\n",
        "\\begin{array}[]{l}{\\tiny {#2}}\\\\\\raise{1.5pt}{{\\tiny{#1}}}\\end{array}}$\n",
        "$\\newcommand{\\xpool}[5]{\n",
        "\\begin{array}[]{r}{\\tiny{#4\\ }}\\\\\\raise{1.5pt}{{\\tiny{#3}}}\\end{array}\n",
        "\\kern{-3pt}\n",
        "\\begin{array}[]{l}{\\LARGE\\bb{P}}\\kern{-6.5pt}\\raise{0.8pt}{{\\tiny{#5}}}\\end{array}\n",
        "\\kern{0pt}\n",
        "\\begin{array}[]{l}{\\tiny {#2}}\\\\\\raise{1.5pt}{{\\tiny{#1}}}\\end{array}}$\n",
        "$\\newcommand{\\xinp}[5]{\n",
        "\\begin{array}[]{r}{\\tiny{#4\\ }}\\\\\\raise{1.5pt}{{\\tiny{#3}}}\\end{array}\n",
        "\\kern{-4pt}\n",
        "\\begin{array}[]{l}{\\LARGE\\cl{I}}\\kern{-4.5pt}\\raise{1.5pt}{{\\tiny{#5}}}\\end{array}\n",
        "\\kern{2pt}\n",
        "\\begin{array}[]{l}{\\tiny {#2}}\\\\\\raise{1.5pt}{{\\tiny{#1}}}\\end{array}}$\n",
        "$\\newcommand{\\xdrop}[5]{\n",
        "\\begin{array}[]{r}{\\tiny{#4\\ }}\\\\\\raise{1.5pt}{{\\tiny{#3}}}\\end{array}\n",
        "\\kern{-4pt}\n",
        "\\begin{array}[]{l}{\\LARGE\\bb{D}}\\kern{-4.5pt}\\raise{1.5pt}{{\\tiny{#5}}}\\end{array}\n",
        "\\kern{0.5pt}\n",
        "\\begin{array}[]{l}{\\tiny {#2}}\\\\\\raise{1.5pt}{{\\tiny{#1}}}\\end{array}}$\n",
        "$\\newcommand{\\xmerge}[5]{\n",
        "\\begin{array}[]{r}{\\tiny{#4\\ }}\\\\\\raise{1.5pt}{{\\tiny{#3}}}\\end{array}\n",
        "\\kern{-4pt}\n",
        "\\begin{array}[]{l}{\\LARGE\\bb{M}}\\kern{-4.5pt}\\raise{1.5pt}{{\\tiny{#5}}}\\end{array}\n",
        "\\kern{2pt}\n",
        "\\begin{array}[]{l}{\\tiny {#2}}\\\\\\raise{1.5pt}{{\\tiny{#1}}}\\end{array}}$\n",
        "$\\newcommand{\\xgeneral}[5]{\n",
        "\\begin{array}[]{r}{\\tiny{#4\\ }}\\\\\\raise{1.5pt}{{\\tiny{#3}}}\\end{array}\n",
        "\\kern{-4pt}\n",
        "\\begin{array}[]{l}{\\LARGE\\bb{Q}}\\kern{-9.0pt}\\raise{4.5pt}{{\\tiny{#5}}}\\end{array}\n",
        "\\kern{2pt}\n",
        "\\begin{array}[]{l}{\\tiny {#2}}\\\\\\raise{1.5pt}{{\\tiny{#1}}}\\end{array}}$\n",
        "$\\def\\ds{\\displaystyle}$\n",
        "$\\def\\ass{\\leftarrow}$\n",
        "$\\def\\od#1#2{\\nabla_{#2}#1}$\n",
        "$\\def\\tod#1#2{\\tp{\\nabla}_{#2}{#1}}$\n",
        "$\\def\\cl#1{{\\cal#1}}$\n",
        "$\\def\\sp#1#2{\\frac{\\partial#1}{\\partial#2}}$\n",
        "$\\def\\eqd{\\doteq}$\n",
        "$\\def\\ra{\\rightarrow}$\n",
        "$\\def\\lra{\\longrightarrow}$\n",
        "$\\def\\ovra#1{\\overset{#1}{\\lra}}$\n",
        "$\\def\\dra{\\overset{\\circ}{\\lra}}$\n",
        "$\\def\\xeq#1{\\overset{#1}{=}}$\n",
        "$\\def\\ov#1{\\overline{#1}}$\n",
        "$\\def\\dotp#1#2{\\left\\langle#1,#2\\right\\rangle}$\n",
        "$\\def\\th#1{\\ov{#1}^{\\intercal}}$\n",
        "$\\def\\rv#1{\\widetilde{#1}}$\n",
        "$\\def\\vars#1#2{\\mathtt{var}_{#1}{\\left[#2\\right]}}$\n",
        "$\\def\\pmodd#1{\\kern{-1mm}\\pmod{#1}}$\n",
        "$\\def\\pdt#1#2#3{\\frac{\\partial^2 #1}{\\partial #2\\partial #3}}$\n",
        "$\\def\\diag#1{\\mathtt{diag}\\left[#1\\right]}$\n",
        "$\\def\\rank#1{\\mathtt{rank}\\left[#1\\right]}$\n",
        "$\\def\\tr#1{\\mathtt{tr}\\left[#1\\right]}$\n",
        "$\\def\\colorr#1#2{\\color{#1}{#2}}$\n",
        "$\\def\\kernn#1{\\kern{#1pt}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCbzt_4Ego1A"
      },
      "source": [
        "#Entropia krzyżowa (skrośna)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1GkrvLBdMSM"
      },
      "source": [
        "##Dywergencja Kullbacka-Leiblera (KL)\n",
        "\n",
        "Dywergencja KL $D_{KL}(p||q)$ to asymetryczna miara dyskryminacyjna dla rozkładów prawdopodobieństwa $p,q:\\cl{A}\\ra\\big[0,1\\big]$:\n",
        "- *przypadek dyskretny*:\n",
        "$D_{KL}(p||q) \\eqd \\ds\\sum_{x\\in\\cl{A}} p(x)\\log\\frac{p(x)}{q(x)}$,\n",
        "- *przypadek ciągły*:\n",
        "$D_{KL}(p||q) \\eqd \\ds\\int_{\\cl{A}} p(x)\\log\\frac{p(x)}{q(x)}dx$.\n",
        "\n",
        "*Uwaga:*<br>Stosujemy następujący zapis dla logarytmów: $\\ln t\\eqd\\log_e t,\\ \\log t\\eqd \\log_2 t\\ .$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvfurmKNljDx"
      },
      "source": [
        "###*Kod KL: przypadek dyskretny*\n",
        "Przjmujemy domyślny zakres dla losowych indeksów dal $K$ klas: $\\cl{A} \\eqd \\{0,\\dots,K-1\\}$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLm1RmAOn3iz"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcMXK9qtpFtf"
      },
      "source": [
        "eps = 1e-30\n",
        "def KL(p,q,base=2,eps=eps):\n",
        "  if base==np.e: return np.sum(p*(np.log(eps+p/(q+eps))))\n",
        "  else: return np.sum(p*(np.log2(eps+p/(q+eps))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92hU9nw25U94"
      },
      "source": [
        "Testowanie"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnzRLRdJnnEo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04f0e0b0-eb91-4513-eee3-06c995ead8df"
      },
      "source": [
        "K = 10; print('number of classes:',K)\n",
        "print('epsilon regularizer:', eps)\n",
        "lbase = np.e # 2\n",
        "print('logarithm base:',lbase)\n",
        "p = np.random.rand(K); p /= np.sum(p)+eps\n",
        "q = np.random.rand(K); q /= np.sum(q)+eps\n",
        "print('p:',p); print('q:',q)\n",
        "print('Kullback-Leibler divergence KL(p,q) for probability distributions p,q:',KL(p,q,base=lbase))\n",
        "print('Kullback-Leibler divergence KL(q,p) for probability distributions q,p:',KL(q,p,base=lbase))\n",
        "print('KL(p,p):',KL(p,p),'KL(q,q):',KL(q,q))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of classes: 10\n",
            "epsilon regularizer: 1e-30\n",
            "logarithm base: 2.718281828459045\n",
            "p: [1.67725265e-01 1.87966046e-01 4.70225174e-02 1.92897607e-01\n",
            " 1.08761754e-01 5.93468753e-02 1.23381233e-04 1.55405328e-01\n",
            " 5.45852715e-02 2.61659544e-02]\n",
            "q: [0.0206206  0.12256533 0.10730532 0.07103052 0.18737581 0.12147213\n",
            " 0.06010995 0.18693299 0.03867242 0.08391493]\n",
            "Kullback-Leibler divergence KL(p,q) for probability distributions p,q: 0.44303391416954546\n",
            "Kullback-Leibler divergence KL(q,p) for probability distributions q,p: 0.601860271223938\n",
            "KL(p,p): 0.0 KL(q,q): 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grgBCfz7IzQY"
      },
      "source": [
        "Eksperymenty wskazują na następujące własności: dywergencja KL jest nieujemna, niesymetryczna i równa się zero gdy rozkłady są identyczne.\n",
        "Czy to jest prawdą w ogólności?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfzpByVAddGl"
      },
      "source": [
        "###Kluczowe nierówności w teorii informacji\n",
        "\n",
        "Logarytm, funkcja taka jak $\\ln t$, jest różniczkowalny, ściśle wklęsłą funkcją w dziedzinie $t>0$. A więc logarytm ma wykres, za wyjątkiem punktu styczności,  poniżej każdej stycznej. Ponieważ logarytm znika w punkcie $t=1$ z nachyleniem jednostkowym (pochodna $\\frac{d\\ ln(t)}{d\\ t}=\\left.\\frac{1}{t}\\right|_{t=1}=1$) to styczna ma równanie $u=t-1.$  Dlatego dla $t>0:$\n",
        "$$\n",
        "\\ln t \\leq t-1\\quad\\lra\\quad\n",
        "\\log t \\leq (t-1)\\log e\\quad \\lra\\quad\n",
        "-\\log t \\geq (1-t)\\log e\n",
        "$$\n",
        "Równość zachodzi wtedy i tylko wtedy $t=1$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wabI5MAAeVXZ"
      },
      "source": [
        "####Dolne ograniczenie dywergencji KL\n",
        "Podstawiając do nierówności $-\\log t\\geq(1-t)\\log e$  jako $t\\eqd\\frac{q(x)}{p(x)}$ otrzymujemy dolne ograniczenie $p(x)\\log\\frac{p(x)}{q(x)}:$\n",
        "$$\\log\\frac{p(x)}{q(x)} = -\\log\\frac{q(x)}{p(x)} \\geq \\frac{p(x)-q(x)}{p(x)}\\cdot\\log e \\quad\\lra\\quad p(x)\\log\\frac{p(x)}{q(x)} \\geq (p(x)-q(x))\\log e\n",
        "$$\n",
        "\n",
        "Równość $p(x)\\log\\frac{p(x)}{q(x)} = (p(x)-q(x))\\log e$ zachodzi tylko gdy $t=1$, tj. wtedy i tylko wtedy gdy $p(x)=q(x)$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DjwYHTYe6VQ"
      },
      "source": [
        "####Dywergencja KL jest zawsze nieujemna\n",
        "Dla dowolnych rozkładów prawdopodobieństwa $p,q$ określonych w tej samej dziedzinie:\n",
        "$$\n",
        "\\boxed{\n",
        "\\begin{array}{l}\n",
        "D_{KL}(p||q)\\geq 0\\\\[3pt]\n",
        "D_{KL}(p||q)=0 \\iff p=q\n",
        "\\end{array}\n",
        "}\n",
        "$$\n",
        "\n",
        "**Ćwiczenie**  Uzasadnij powyższe własności dywergencji KL: nieujemność i identyczność nierozróżnialnych.<br>\n",
        "*Wskazówka:* Skorzystaj z powyżej wyprowadzonej nierówności<br>\n",
        " $p(x)\\log\\frac{p(x)}{q(x)} \\geq (p(x)-q(x))\\log e$<br>\n",
        " oraz z własności jedynki<br>\n",
        " $\\ds\\sum_{x\\in\\cl{A}}p(x)=1$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8sxDNhIgNAv"
      },
      "source": [
        "###Entropia a dywergencja KL\n",
        "\n",
        "Entropię $H[p]$ rozkładu $p:\\cl{A}\\ra[0,1]$ jest wartością oczekiwaną miary informacji $\\log\\frac{1}{p(x)},$ $x\\in\\cl{A}$:\n",
        "$$\n",
        "H(p)\\eqd\\ds\\sum_{x\\in\\cl{A}}p(x)\\log\\frac{1}{p(x)}\n",
        "$$\n",
        "Niech $r^{\\ast}$ będzie rozkładem równomiernym: $r^{\\ast}(x)\\eqd\\frac{1}{|\\cl{A}|},$ dla dowolnego  $x\\in\\cl{A}$.\n",
        "\n",
        "**Ćwiczenie**\n",
        "\n",
        "Uzasadnij dla dowolnego rozkładu $p$ na dziedzinie $\\cl{A}:$\n",
        "$$\n",
        "\\begin{array}{rcl}\n",
        "H(p) & = & \\log|\\cl{A}|-D_{KL}(p||r^{\\ast})\\\\[5pt]\n",
        "H(r^{\\ast}) & = & \\log|\\cl{A}|\\\\[5pt]\n",
        "D_{KL}(p||r^{\\ast}) & = &  H(r^{\\ast})-H(p)\n",
        "\\end{array}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_8-W1AhHefZ"
      },
      "source": [
        "###Entropia skrośna a dywergencja KL\n",
        "\n",
        "Entropię skrośną $H(p,q)$  rozkładów $p,q:\\cl{A}\\ra[0,1]$ określamy jako wartość oczekiwaną informacji $\\log\\frac{1}{q(x)}$ w rozkładzie $q$  uśrednianą zgodnie z rozkładem $p$:\n",
        "$$\n",
        "H(p,q)\\eqd\\ds\\sum_{x\\in\\cl{A}}p(x)\\log\\frac{1}{q(x)}\n",
        "$$\n",
        "\n",
        "**Ćwiczenie**\n",
        "\n",
        "Uzasadnij, że dla dowolnych rozkładów $p,q$ on $\\cl{A}:$\n",
        "$$D_{KL}(p||q) = H(p,q) - H(p)\\ \\text{ oraz }\\ H(p,q) = D_{KL}(p||q) + H(p)$$\n",
        "\n",
        "*Wnioski:*\n",
        "- Jesli $p$ jest docelowym rozkładem prawdopopdobieństwa a rozkład $q$ jest predykcją is tego rozkładu wtedy  entropia skrośna $H(p,q)$ może być stosowana na równi z optymalizacją dywergencji KL $D_{KL}(p||q)$.\n",
        "- Jeśli docelowy rozkład jest skupiony na indeksie $k$, tj. gdy $p(x)\\eqd\\delta(x-k)$, to $H(p)=0$ i wtedy błąd predykcji równa się entropii skrośnej, która z kolei równa się informacji indeksu $k$-tej klasy:\n",
        "$$p(x)\\eqd\\delta(x-k)\\quad \\lra\\quad D_{KL}(p||q)=H(p,q)=\\log\\frac{1}{q(k)}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNwNYBkQPQzm"
      },
      "source": [
        "Kod entropii i entropii skrośnej"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1CjWlBBPPte"
      },
      "source": [
        "def entropy(p): return np.sum(p*np.log2(eps+1./(eps+p)))\n",
        "def cross_entropy(p,q): return np.sum(p*np.log2(eps+1./(eps+q)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfaDqGs3P5RY"
      },
      "source": [
        "Testowanie"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpgtJWliP7XF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ff9e35f-c124-4585-e19f-993a3e6b5688"
      },
      "source": [
        "K = 10; print('number of classes:',K)\n",
        "print('epsilon regularizer:', eps)\n",
        "p = np.random.rand(K); p /= np.sum(p)+eps\n",
        "q = np.random.rand(K); q /= np.sum(q)+eps\n",
        "print('p:',p); print('q:',q)\n",
        "print('Entropy H(p) for probability distribution p:',entropy(p))\n",
        "print('Cross entropy H(p,q) for probability distributions p,q:',cross_entropy(p,q))\n",
        "print('KL(p,q):',KL(p,q))\n",
        "print('check KL(p,q) - (H(p,q) - H(p)) = 0?:',KL(p,q)-cross_entropy(p,q)+entropy(p))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of classes: 10\n",
            "epsilon regularizer: 1e-30\n",
            "p: [0.13756942 0.12042266 0.08853904 0.01633787 0.07960555 0.06675912\n",
            " 0.16917345 0.1032493  0.13826387 0.08007974]\n",
            "q: [0.09325098 0.07821593 0.19844754 0.06222918 0.01778978 0.16396631\n",
            " 0.08948965 0.23180494 0.01294528 0.05186041]\n",
            "Entropy H(p) for probability distribution p: 3.177657486613441\n",
            "Cross entropy H(p,q) for probability distributions p,q: 3.8383201052872136\n",
            "KL(p,q): 0.6606626186737723\n",
            "check KL(p,q) - (H(p,q) - H(p)) = 0?: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3KZcqgVzyH6"
      },
      "source": [
        "###Normalizacja *SoftMax* i entropia skrośna"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFPGSh-Puld0"
      },
      "source": [
        "Niech $y\\eqd F(x)\\inv{K}$ będzie wektorem wyników dla wejścia $x$, które reprezentuje klasę o indeksie $k^{\\ast}\\eqd t(x)\\in[K].$<br>\n",
        "Wtedy funkcja strat w kombinacji *SoftMax+CrossEntropy* jest definiowana następująco:\n",
        "- *Wynik wejściowy* $y$: $y[k]$ - artość wynikowa dla klasy $k$, $k\\in[K]$.\n",
        "- *Docelowy rozkład prawdopodobieństwa* $q^{\\ast}$: $q^{\\ast}[k]\\eqd \\delta[k-k^{\\ast}],\\ k\\in[K]$.\n",
        "\n",
        "*Normalizacja SoftMax*\n",
        "\n",
        "Dla $k\\in[K]$:\n",
        "1. $\\ds z[k]\\eqd e^{y[k]}$ $-$ przechodzimy do skali, w której wynik $y[k]$ ma charakter logarytmiczny,\n",
        "1. $\\ds p_y[k]\\eqd \\frac{z[k]}{\\ds\\sum_{i\\in[K]}z[i]}$ $-$ normalizacja wyniku do rozkładu prawdopodobieństwa\n",
        "\n",
        "*Entropia skrośna*\n",
        "\n",
        "- *Funkcja strat* = $\\frac{1}{\\ln 2}\\cdot$ entropia skrośna rozkładów $q^{\\ast}$ oraz $p_y$:\n",
        "$$\\cl{E}(y) \\eqd -\\ln p_y[k^{\\ast}]$$\n",
        "- Jawna formuła funkcji strat względem wektora wyników $y\\inv{K}:$\n",
        "$$\n",
        "\\cl{E}(y) = -\\ln\\frac{e^{y[k^{\\ast}]}}{\\ds\\sum_{k\\in[K]}e^{y[k]}}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hY84H6IRv0TI"
      },
      "source": [
        "*Uwagi*:\n",
        "- Faktycznie powyższa funkcja strat nie jest stricte entropią skrośną, ale jej skalowalną przez $\\frac{1}{\\log e}$ wersją.\n",
        "- Ponieważ wyrażenie $-\\ln p_y[k^{\\ast}]=\\left(-\\log p_y[k^{\\ast}]\\right)\n",
        "\\cdot\\frac{1}{\\log e}$\n",
        "jest proporcjonalne do miary informacji w sensie Shannona, można interpretować tę funkcję strat jako informację Shannona *Shannon information* indeksu $k^{\\ast}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_NXtPNKsVGz"
      },
      "source": [
        "Kod dla *SoftMax*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5_lrKOnsep7"
      },
      "source": [
        "def softmax(y): p = np.exp(y); return p/np.sum(p)\n",
        "def bayes_principle(y): p = softmax(y); k = np.argmax(p); return k, p[k]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6oq7qMQvvrr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2f13848-82f4-4b4f-8cc7-2144b340bae0"
      },
      "source": [
        "K = 10; y = np.random.randn(K)\n",
        "print('number of classes:',K); print('random scores y:',y)\n",
        "p = softmax(y); print('their SoftMax normalization probabilities p:',p)\n",
        "print('check property of one:',np.sum(p))\n",
        "k, p_k = bayes_principle(y)\n",
        "print('class by Bayesian principle:',k,'\\nwith probability:',p_k)\n",
        "print('class from scores:',np.argmax(y))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of classes: 10\n",
            "random scores y: [ 2.63650409 -0.51797254 -0.15387961  0.24233611 -0.46747123  1.18941102\n",
            "  0.2773767   0.01653643  0.59267969  0.22630638]\n",
            "their SoftMax normalization probabilities p: [0.53703721 0.0229104  0.03297282 0.04900388 0.02409711 0.1263397\n",
            " 0.05075144 0.03909911 0.06956371 0.04822462]\n",
            "check property of one: 0.9999999999999999\n",
            "class by Bayesian principle: 0 \n",
            "with probability: 0.5370372127847276\n",
            "class from scores: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN73yKO_v5nc"
      },
      "source": [
        "###Gradient funkcji *SoftMax+KL*\n",
        "**Twierdzenie** (o gradiencie Softamx+KL)<br>\n",
        "Zakładając powyższą definicję funkcji strat $\\cl{E}(y), y\\inv{K},$ jej gradient względem wyniku $y$ może byc wyrażony przez różnicę między rozkładem *SoftMax*  $p_y$ a rozkładem docelowym $q^{\\ast}$:\n",
        "$\n",
        "\\boxed{\\od{\\cl{E}}{y} = p_y-q^{\\ast}}\n",
        "$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thy5u-5DFHRr"
      },
      "source": [
        "*Dowód:*<br>\n",
        "Zaczynamy od policzenia pochodnych $\\sp{\\cl{E}(y)}{y[i]}$ for any $i\\in[K]$:\n",
        "$$\n",
        "\\begin{array}{l}\n",
        "\\ds\\sp{\\cl{E}}{y[i]} =\n",
        "\\sp{}{y[i]}\\left[-\\ln\\frac{e^{y[k^{\\ast}]}}{\\ds\\sum_{k\\in[K]}e^{y[k]}}\\right]=\\\\[10pt]\n",
        "-\\left(\\frac{\\ds\\sum_{k\\in[K]}e^{y[k]}}{e^{y[k^{\\ast}]}}\\right)\\cdot\n",
        "\\frac{\\ds\\sp{e^{y[k^{\\ast}]}}{y[i]}\\cdot\n",
        "\\left(\\ds\\sum_{k\\in[K]}e^{y[k]}\\right)-e^{y[k^{\\ast}]}\\cdot\\sp{}{y[i]}\\left(\\ds\\sum_{k\\in[K]}e^{y[k]}\\right)}\n",
        "{\\left(\\ds\\sum_{k\\in[K]}e^{y[k]}\\right)^2}\n",
        "\\end{array}\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVS8UIB_MQpH"
      },
      "source": [
        "Teraz, rozważmy przypadek $i\\neq k^{\\ast}$:\n",
        "$$\n",
        "\\begin{array}{l}\n",
        "\\ds\\sp{\\cl{E}}{y[i]} =\n",
        "-\\left(\\frac{\\ds\\sum_{k\\in[K]}e^{y[k]}}{e^{y[k^{\\ast}]}}\\right)\\cdot\n",
        "\\frac{\\overbrace{\\ds\\sp{e^{y[k^{\\ast}]}}{y[i]}}^{\\ds 0}\\cdot\n",
        "\\left(\\ds\\sum_{k\\in[K]}e^{y[k]}\\right)-e^{y[k^{\\ast}]}\\cdot\n",
        "\\overbrace{\\ds\\sp{}{y[i]}\\left(\\ds\\sum_{k\\in[K]}e^{y[k]}\\right)}^{\\ds e^{y[i]}}\n",
        "}\n",
        "{\\left(\\ds\\sum_{k\\in[K]}e^{y[k]}\\right)^2}\\\\[5pt]\n",
        "\\lra\\quad \\ds\\sp{\\cl{E}}{y[i]} = \\frac{e^{y[i]}}{\\ds\\sum_{k\\in[K]}e^{y[k]}}\n",
        "\\quad\\lra\\quad \\ds\\sp{\\cl{E}}{y[i]} =  p_y[i]\\ \\text{ if } i\\neq k^{\\ast}\n",
        "\\end{array}\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmBpc1XAP-le"
      },
      "source": [
        "Końcowy przypadek $i=k^{\\ast}$:\n",
        "$$\n",
        "\\begin{array}{l}\n",
        "\\ds\\sp{\\cl{E}}{y[k^{\\ast}]} =\n",
        "-\\left(\\frac{\\ds\\sum_{k\\in[K]}e^{y[k]}}{e^{y[k^{\\ast}]}}\\right)\\cdot\n",
        "\\frac{\\overbrace{\\ds\\sp{e^{y[k^{\\ast}]}}{y[k^{\\ast}]}}^{\\ds e^{y[k^{\\ast}]}}\\cdot\n",
        "\\left(\\ds\\sum_{k\\in[K]}e^{y[k]}\\right)-e^{y[k^{\\ast}]}\\cdot\n",
        "\\overbrace{\\ds\\sp{}{y[k^{\\ast}]}\\left(\\ds\\sum_{k\\in[K]}e^{y[k]}\\right)}^{\\ds e^{y[k^{\\ast}]}}\n",
        "}\n",
        "{\\left(\\ds\\sum_{k\\in[K]}e^{y[k]}\\right)^2}\\\\[5pt]\n",
        "\\lra\\quad \\ds\\sp{\\cl{E}}{y[k^{\\ast}]} = \\frac{e^{y[k^{\\ast}]}}{\\ds\\sum_{k\\in[K]}e^{y[k]}}-1\n",
        "\\quad\\lra\\quad \\ds\\sp{\\cl{E}}{y[k^{\\ast}]} =  p_y[k^{\\ast}]-1\\ \\text{ if } i\\neq k^{\\ast}\n",
        "\\end{array}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aV4EeyedTd18"
      },
      "source": [
        "Podsumowując:\n",
        "$$\n",
        "\\begin{array}{l}\n",
        "\\ds\\sp{\\cl{E}}{y[i]}\n",
        "=\n",
        "\\left\\{\n",
        "\\begin{array}{ll}\n",
        "   p_y[i] = p_y[i]-\\delta(i-k^{\\ast}) = p_y[i] - q^{\\ast}[i]  & \\text{ if } i\\neq k^{\\ast}\\\\\n",
        "   p_y[i]-1 = p_y[i] - \\delta(i-k^{\\ast}) = p_y[i] - q^{\\ast}[i]& \\text{ if } i= k^{\\ast}\n",
        "\\end{array}\n",
        "\\right.\\\\[5pt]\n",
        "\\quad\\lra\\quad \\od{\\cl{E}}{y} = p_y-q^{\\ast}\\quad \\boxed{QED}\n",
        "\\end{array}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwfXEhS9xu1P"
      },
      "source": [
        "Kod gradientu dla *SoftMax+KL*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKi3Iyt2xtZd"
      },
      "source": [
        "def gradient_soft_KL(y,k): p = softmax(y); p[k] -= 1.; return p"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W70Zfea0xtZu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d7e6e15-8645-429a-9bec-ca59716632ae"
      },
      "source": [
        "K = 10; y = np.random.randn(K); k = np.random.randint(K)\n",
        "print('number of classes:',K); print('random class index:',k)\n",
        "print('random scores y:',y)\n",
        "g = gradient_soft_KL(y,k)\n",
        "print('gradient of SoftMax+KL loss:', g)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of classes: 10\n",
            "random class index: 2\n",
            "random scores y: [ 0.55294501 -0.81546323 -0.04831704  0.48515335  0.96683605 -0.79536278\n",
            "  0.23909972  1.65207774  1.31829224  1.83778058]\n",
            "gradient of SoftMax+KL loss: [ 0.07140069  0.01817232 -0.96086389  0.06672075  0.10800728  0.01854128\n",
            "  0.05216764  0.21431359  0.15349298  0.25804737]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxKrARmh3wkM"
      },
      "source": [
        "###Gradient w TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aGgPZG_WBHg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3ce3cd5-3639-4be1-c501-e9859066f03b"
      },
      "source": [
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "TAs5UD_6Fbng",
        "outputId": "d6839ce2-d925-40d1-e7b4-c5e9536b01ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.14.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7XQzFIGIgNK"
      },
      "source": [
        "def KL_tf(p,q): return tf.math.reduce_sum(p*tf.math.log(eps+(p/(eps+q))))\n",
        "def softmax_tf(y): p = tf.math.exp(y); return p/tf.math.reduce_sum(p)\n",
        "def forward_tf(y,q): p = softmax_tf(y); return KL_tf(q,p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJtfORQoI4b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36d1e85d-fa25-4237-dfbe-26a56677352e"
      },
      "source": [
        "q_star = np.zeros(len(y)); q_star[k] += 1.\n",
        "y_tf = tf.Variable(y); q_tf = tf.Variable(q_star)\n",
        "print('scores in numpy:',y)\n",
        "print('scores wrapped in tensorflow:',y_tf)\n",
        "print('class categoric vector:',q_tf)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "scores in numpy: [ 0.55294501 -0.81546323 -0.04831704  0.48515335  0.96683605 -0.79536278\n",
            "  0.23909972  1.65207774  1.31829224  1.83778058]\n",
            "scores wrapped in tensorflow: <tf.Variable 'Variable:0' shape=(10,) dtype=float64, numpy=\n",
            "array([ 0.55294501, -0.81546323, -0.04831704,  0.48515335,  0.96683605,\n",
            "       -0.79536278,  0.23909972,  1.65207774,  1.31829224,  1.83778058])>\n",
            "class categoric vector: <tf.Variable 'Variable:0' shape=(10,) dtype=float64, numpy=array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16uMHe7cKBnk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c20e7974-1201-4d1f-8db9-ab041a7ad61f"
      },
      "source": [
        "e = forward_tf(y_tf,q_tf)\n",
        "print('Softmax+KL error:',e)\n",
        "print('Note: in TensorFlow/Keras this loss function is implemented by:',\n",
        "      'tf.keras.losses.CategoricalCrossentropy')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Softmax+KL error: tf.Tensor(3.2407097421815387, shape=(), dtype=float64)\n",
            "Note: in TensorFlow/Keras this loss function is implemented by: tf.keras.losses.CategoricalCrossentropy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCFWtWK9HI3S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8690845-84a0-44f4-c164-4e8abb51bdb7"
      },
      "source": [
        "with tf.GradientTape() as gt: gt.watch(y_tf); e = forward_tf(y_tf,q_tf)\n",
        "de_dy = gt.gradient(e, y_tf)\n",
        "print('gradient of error by TensorFlow:',de_dy)\n",
        "print('difference of gradient from (p-q_star) formula with TensorFlow one:',\n",
        "      np.linalg.norm(g-de_dy)/K)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gradient of error by TensorFlow: tf.Tensor(\n",
            "[ 0.07140069  0.01817232 -0.96086389  0.06672075  0.10800728  0.01854128\n",
            "  0.05216764  0.21431359  0.15349298  0.25804737], shape=(10,), dtype=float64)\n",
            "difference of gradient from (p-q_star) formula with TensorFlow one: 3.4694469519536144e-19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ćwiczenie A\n",
        "\n",
        "Dostosuj powyższe funkcje do podstawy logarytmu naturalnego i podstawy 2.\n",
        "\n"
      ],
      "metadata": {
        "id": "PzfLAePjWCQM"
      }
    }
  ]
}